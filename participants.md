---
layout: page
title: "Speakers"
tags: [participants]
image:
  feature: pierce2.jpg
  credit: 
  creditlink: 
share: true
---
# Confirmed Workshops

## Carlos Dominguez

<figure>
	<a href="/images/dominguez.jpg"><img width="400" src="/images/dominguez.jpg" alt=""></a>
</figure>

[Carlos Dominguez](http://charlossound.wordpress.com/) is an alum of Dartmouth's Digital Musics program and a Research Assistant in Dartmouth's Film & Media Studies Department.

### Description
Carlos will give a tutorial on making a hybrid analog/digital device for reading light data for creative expression.

## Alex Dupuis

<figure>
	<a href="/images/dupuis.png"><img width="400" src="/images/dupuis.png" alt=""></a>
</figure>

[Alex Dupuis](http://www.alexanderdupuis.com/about.php) is a composer, animator, and performer. His work investigates the intersections between experimental music and film, focusing particularly on theories of audiovisual perception and transduction of information between light and sound. He performs as a guitarist, as well as with audiovisual instruments and software systems of his own design. He is currently pursuing a PhD in Brown University's Multimedia & Electronic Music Experiments program.

### Description

Alex will discuss digital raster scanning as audiovisual synthesis.

## James Kerr

<figure>
	<a href="/images/kerr.jpg"><img width="400" src="/images/kerr.jpg" alt=""></a>
</figure>

James Kerr is an animator who creates GIFs using Renaissance art and graphic illustrations. His humorous work treats subjects such as religion, class, entertainment, and modernism by juxtaposing Renaissance art with graphic depictions drawing from pop cultural references. [His work](http://scorpiondagger.com/) has been featured on [Boing Boing](http://boingboing.net/2014/08/04/the-amazing-renaissance-art-gi.html) and other blogs.

### Description
James will talk about his compositional process, along with a discussion of interactive (GIF) art in live settings.

## Jodie Mack

<figure>
	<a href="/images/mack.jpg"><img width="400" src="/images/mack.jpg" alt=""></a>
</figure>

[Jodie Mack](http://www.jodiemack.com/) is an experimental animator and an Assistant Professor teaching animation at the Department of Film & Media Studies at Dartmouth College.

### Description 
Her tutorial description is forthcoming.

## Kyle McDonald

<figure>
	<a href="/images/mcdonald.png"><img width="400" src="/images/mcdonald.png" alt=""></a>
</figure>

[Kyle McDonald](http://kylemcdonald.net/) is a computational artist who is an active contributor to the [openFrameworks](http://www.openframeworks.cc/) community. He has been a resident at CMU's [Frank-Ratchye Studio for Creative Inquiry](http://studioforcreativeinquiry.org/) and is an Adjunct Professor at NYU's ITP.

### "Light Leaks"
["Light Leaks"](http://vimeo.com/66167082) is an immersive installation that uses mirror balls to reflect light from multiple projectors and cover the entirety of space. The mapping of every reflection is computed using a technique called structured light, which allows us to recover the 3d position and orientation of every reflection, as well as its origin in screen space. We will cover the general architecture of the system, including the calibration phase, and a brief discussion of related work and next steps.

## Scott Petersen

<figure>
	<a href="/images/peterson.jpg"><img width="400" src="/images/peterson.jpg" alt=""></a>
</figure>

[Scott Petersen](http://scacinto.wordpress.com/bios/sca-c-i-n-t-o/) (aka s c a c i n t o | s k ɑ ʃ ɪ n t o ʊ | ) is a composer, performer, electronic musician
and laptop improviser. His current research and work revolves around open music technologies, improvisational electronic music, analog electronic instrument design, and experimental music programming. His output is diverse and includes works for large orchestra, small ensemble with and without electronics, works for large homogeneous instrumental groups, film/animation, sound installation, and laptop improvisation with custom designed hardware and software interfaces. Scott has performed and had his work performed and exhibited throughout the United States and Europe. He is a founding member of both the collaborative New Haven Electronic Music Composers group (El MuCo) and of [MakeHaven](http://makehaven.org/), a DIY maker space, and he founded and curates the FridayNightThing, an ongoing new music and arts gathering. Scott currently works as the Assistant Director of the [Yale Music Technology Labs](http://yalmust.yale.edu/) and as the director of the YalMusT Open Music Initiative in the Yale Department of Music.

### Systems and Methods of Performative Synthesis: Considerations for Improvising with Code

This workshop will provide the attendee with an introduction to several systems and methods for improvising with code, focussing specifically on the strengths and weaknesses of different approaches to live audio synthesis. The workshop will use the audio programming language SuperCollider for all examples, but the concepts are portable to other languages.

Key topics will include: data entry and interaction methods, communication protocols, language­specific goodies (UGens), one­liners, strengths and weaknesses of syntactic sugar, hardware considerations for real­time synthesis and more.

You may find it helpful to install SuperCollider before attending the workshop as example files, documentation and further reading will be provided beforehand.

## Andy Sarroff

<figure>
	<a href="/images/sarroff.jpg"><img width="400" src="/images/sarroff.jpg" alt=""></a>
</figure>

[Andy Sarroff](http://www.cs.dartmouth.edu/~sarroff/) is a doctoral student in Computer Science at Dartmouth.

### Description

He will give a tutorial on building neual networks for musical sound synthesis.

## Kevin Woods

<figure>
	<a href="/images/woods.jpg"><img width="400" src="/images/woods.jpg" alt=""></a>
</figure>

Kevin Woods is a graduate student researcher at Josh McDermott's [Laboratory for Computational Audition](http://mcdermottlab.mit.edu/).

### Description

His talk description is forthcoming but will likely focus on auditory scene analysis for creative audio synthesis.

# Other Speakers (Unconfirmed)

## Joan Bruna

<figure>
	<a href="/images/bruna.jpg"><img width="400" src="/images/bruna.jpg" alt=""></a>
</figure>

[Joan Bruna](http://cims.nyu.edu/~bruna/Home.html) Is a postdoctoral researcher in applied mathematics in Yann LeCun’s group, in the Courant Institute, New York. Joan's research focuses on scattering transforms for classification and he has demonstrated scattering transforms on algorithms for [audio texture synthesis](http://cims.nyu.edu/~bruna/Audio_Texture_Synthesis.html).

### Description

Forthcoming.

## Jason Hockman

<figure>
	<a href="/images/hockman.jpg"><img width="400" src="/images/hockman.jpg" alt=""></a>
</figure>

[Jason Hockman](http://www.music.mcgill.ca/~hockman/) recently completed his PhD on a technological and ethnomusicological study of breakbeats at McGill University. His research has focused on musical rhythm analysis and synthesis.

### Description
Forthcoming.

## Parag Mital

<figure>
	<a href="/images/mital.jpg"><img width="400" src="/images/mital.jpg" alt=""></a>
</figure>

[Parag Mital](http://pkmital.com/home/) is a postdoctoral researcher at Dartmouth college. He currently works on mapping musical audio to brain images using fMRI data.

### Computational Audiovisual Scene Synthesis.

This thesis attempts to open a dialogue around fundamental questions of perception such as: how do we represent our ongoing auditory or visual perception of the world using our brain; what could these representations explain and not explain; and how can these representations eventually be modeled by computers? Rather than answer these questions scientifically, we will attempt to develop a computational arts practice presenting these questions to participants. The approach this thesis takes is computational scene synthesis: a computationally generative collage process where the units of the collage are built using perceptually-inspired representations. We explain how scene synthesis is built in detail and relate it to an existing lineage of collage-based practitioners. Then, working in auditory and visual domains separately, in order to bring questions of perception to the experience of the artwork, this thesis makes significant interdisciplinary strides from reviewing fundamental issues in perception in terms of experimental psychology and cognitive neuroscience, to formulating and developing perceptually-inspired computational models of large databases of audiovisual material, to finally developing these models with a computationally generative collage-based arts practice. Two final practical outputs using audiovisual scene synthesis will be explored: (1) a short film series which attempts to recreate the number 1 video of the week on YouTube using only the audiovisual content from the remaining top 10 videos; and (2) a real-time augmented reality experience presented through a virtual reality headset and headphones presenting a scene synthesis of a participant's surroundings using only previously learned audiovisual fragments. Results from both outputs demonstrate the ability for scene synthesis to provoke meaningful engagements with one's own process of perception. The results further demonstrate that scene synthesis is capable of highlighting both theoretical and practical gaps in our current understanding of human perception and their computational implementations.

## Makino Takashi

<figure>
	<a href="/images/takashi.gif"><img width="400" src="/images/takashi.gif" alt=""></a>
</figure>

[Makino Takashi](http://makinotakashi.net/) is a Japanese experimental film artist.

### Description
Forthcoming

## Spencer Topel

<figure>
	<a href="/images/topel.jpg"><img width="400" src="/images/topel.jpg" alt=""></a>
</figure>

[Spencer Topel](http://www.spencertopel.com/) is a composer and Assistant Professor of Music at Dartmouth College. His works include three string quartets, five symphonic works, and numerous chamber and electronic works and sound-art installations. 

### Description

Forthcoming.

## James Traer

<figure>
	<a href="/images/traer.jpg"><img width="400" src="/images/traer.jpg" alt=""></a>
</figure>

James Traer is a postdoctoral researcher at Josh McDermott's [Laboratory for Computational Audition](http://mcdermottlab.mit.edu/). He is a physicist and oceanographer whose prior work focused on extracting information from ambient noise in the ocean and Earth's crust. He is currently at MIT studying acoustic reverberation.


### Synthesis of realistic (and wildly unrealistic) sounding impulse responses from environmental statistics.

Every room is different and hence has a set of unique impulse responses associated with each possible source-listener orientation.  However, the percept of reverberation may be similar across a wide range of rooms and configurations therein.  This suggests that the perceptually important features of reverberation may be independent of the details of the fine-structure of the impulse response (determined by the exact configuration and properties of the room) and rather may depend more strongly on the bulk statistics of the impulse response time series (determined by features such as room size and average reflectance of all surfaces).  We have measured real-world impulse responses in over 400 real-world spaces, both indoor and outdoor, including rooms large and small.  We have measured the statistics of these impulse responses and by imposing these statistics on Gaussian random noise we can quickly and easily synthesize realistic sounding reverberation.  Moreover, if we set our synthesis algorithm to intentionally violate the statistics we have measured in the real-world, the resulting “reverberant” signal sounds noticeably synthetic and unnatural.  This implies that the human brain uses the statistics of real-world impulse responses to separate signal arriving at the ear into contributions from the sound source and echoes from environmental reverberation. 


